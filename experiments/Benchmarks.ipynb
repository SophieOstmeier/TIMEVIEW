{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[No tuning, using default hyperparameters]\n",
      "[Training for 10 trials with best hyperparameters]\n",
      "[Training trial 1/10] seed: 1826701614\n",
      "[Training trial 2/10] seed: 1367864806\n",
      "[Training trial 3/10] seed: 1097657231\n",
      "[Training trial 4/10] seed: 579362555\n",
      "[Training trial 5/10] seed: 661058651\n",
      "[Training trial 6/10] seed: 87989972\n",
      "[Training trial 7/10] seed: 161576974\n",
      "[Training trial 8/10] seed: 35492826\n",
      "[Training trial 9/10] seed: 376383645\n",
      "[Training trial 10/10] seed: 1746484539\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB</td>\n",
       "      <td>0.004718</td>\n",
       "      <td>8.673617e-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name      mean           std\n",
       "0  XGB  0.004718  8.673617e-19"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from benchmarks.benchmark import run_all_benchmarks, GAM, XTYDataset, TTSBenchmark, XGBBenchmark\n",
    "from tts.data import synthetic_tumor_data\n",
    "from tts.config import Config\n",
    "\n",
    "seed = 0\n",
    "\n",
    "X, ts, ys = synthetic_tumor_data(2000,20,1.0,0.0,seed=seed,equation='wilkerson')\n",
    "dataset = XTYDataset(X, ts, ys)\n",
    "\n",
    "gam = GAM()\n",
    "\n",
    "xgb = XGBBenchmark()\n",
    "\n",
    "config = Config(n_features=4, n_basis=5, T=1, seed=seed, dataloader_type='iterative')\n",
    "tts = TTSBenchmark(config)\n",
    "\n",
    "run_all_benchmarks(dataset, [xgb], seed=seed, n_trials=1,n_tune=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_all_benchmarks(dataset, [gam], seed=seed, n_trials=10,n_tune=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('../')\n",
    "from get_dataset import save_dataset, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A dataset description file with this name already exists at dataset_descriptions\\synthetic_tumor_wilkerson_1.json.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m synthetic_tumor_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msynthetic_tumor_wilkerson_1\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m save_dataset(synthetic_tumor_name, \u001b[39m'\u001b[39;49m\u001b[39mSyntheticTumorDataset\u001b[39;49m\u001b[39m'\u001b[39;49m, {\u001b[39m'\u001b[39;49m\u001b[39mn_samples\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m2000\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mn_time_steps\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m20\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtime_horizon\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m1.0\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mnoise_std\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m0.0\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m0\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mequation\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mwilkerson\u001b[39;49m\u001b[39m'\u001b[39;49m}, dataset_description_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdataset_descriptions\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\PhD\\Papers\\Transparent Time Series\\tts\\experiments\\get_dataset.py:16\u001b[0m, in \u001b[0;36msave_dataset\u001b[1;34m(dataset_name, dataset_builder, dataset_dictionary, notes, dataset_description_path)\u001b[0m\n\u001b[0;32m     14\u001b[0m path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dataset_description_path, dataset_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.json\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(path):\n\u001b[1;32m---> 16\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mA dataset description file with this name already exists at \u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m dataset_description \u001b[39m=\u001b[39m {\n\u001b[0;32m     19\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdataset_name\u001b[39m\u001b[39m'\u001b[39m: dataset_name,\n\u001b[0;32m     20\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdataset_builder\u001b[39m\u001b[39m'\u001b[39m: dataset_builder,\n\u001b[0;32m     21\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdataset_dictionary\u001b[39m\u001b[39m'\u001b[39m: dataset_dictionary,\n\u001b[0;32m     22\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mnotes\u001b[39m\u001b[39m'\u001b[39m: notes\n\u001b[0;32m     23\u001b[0m }\n\u001b[0;32m     24\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[1;31mValueError\u001b[0m: A dataset description file with this name already exists at dataset_descriptions\\synthetic_tumor_wilkerson_1.json."
     ]
    }
   ],
   "source": [
    "\n",
    "save_dataset(synthetic_tumor_name, 'SyntheticTumorDataset', {'n_samples': 2000, 'n_time_steps': 20, 'time_horizon': 1.0, 'noise_std': 0.0, 'seed': 0, 'equation': 'wilkerson'}, dataset_description_path='dataset_descriptions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_tumor_name = 'synthetic_tumor_wilkerson_1'\n",
    "synthetic_tumor_dataset = load_dataset(synthetic_tumor_name, dataset_description_path='dataset_descriptions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, ts, ys = synthetic_tumor_dataset.get_X_ts_ys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\PhD\\Papers\\Transparent Time Series\\tts\\experiments\\benchmark.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'timestamp':timestamp, 'dataset_name':dataset_name, 'n_trials':n_trials, 'n_tune':n_tune, 'train_size':dataset_split[0], 'val_size':dataset_split[1], 'seed':seed}, ignore_index=True)\n",
      "\u001b[32m[I 2023-04-23 17:52:36,944]\u001b[0m A new study created in memory with name: no-name-e2f3a4c1-f19e-4264-b6ad-ad5ca9f44f2a\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tuning for 1 trials]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-23 17:52:39,103]\u001b[0m Trial 0 finished with value: 0.06011846565687811 and parameters: {'n_estimators': 553, 'eta': 0.02693883019285411, 'min_child_weight': 7, 'max_depth': 6, 'gamma': 2.4504079607415994e-05, 'subsample': 0.6813047017599905, 'colsample_bytree': 0.49382849013642327, 'lambda': 0.13620216352659412}. Best is trial 0 with value: 0.06011846565687811.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Best hyperparameter configuration]:\n",
      "{'n_estimators': 553, 'eta': 0.02693883019285411, 'min_child_weight': 7, 'max_depth': 6, 'gamma': 2.4504079607415994e-05, 'subsample': 0.6813047017599905, 'colsample_bytree': 0.49382849013642327, 'lambda': 0.13620216352659412}\n",
      "[Tuning complete], saved tuning results to benchmarks\\2023-04-23T17-52-36\\XGB\\tuning\n",
      "[Training for 1 trials with best hyperparameters]\n",
      "[Training trial 1/1] seed: 1826701614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-23 17:52:41,990]\u001b[0m A new study created in memory with name: no-name-51597d20-cb6b-481d-8a6e-c9d65a7be0f3\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tuning for 1 trials]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-23 17:52:46,734]\u001b[0m Trial 0 finished with value: 0.13493596435532373 and parameters: {'max_bins': 339, 'validation_size': 0.2430378732744839, 'outer_bags': 11, 'inner_bags': 4, 'learning_rate': 0.00703573702872215, 'max_leaves': 4, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.13493596435532373.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Best hyperparameter configuration]:\n",
      "{'max_bins': 339, 'validation_size': 0.2430378732744839, 'outer_bags': 11, 'inner_bags': 4, 'learning_rate': 0.00703573702872215, 'max_leaves': 4, 'min_samples_leaf': 2}\n",
      "[Tuning complete], saved tuning results to benchmarks\\2023-04-23T17-52-36\\GAM\\tuning\n",
      "[Training for 1 trials with best hyperparameters]\n",
      "[Training trial 1/1] seed: 1826701614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-23 17:52:51,971]\u001b[0m A new study created in memory with name: no-name-1ad47397-337e-40a6-9547-c271e7773900\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tuning for 1 trials]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\krkac\\Miniconda3\\envs\\tts\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:176: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "Missing logger folder: benchmarks\\2023-04-23T17-52-36\\TTS\\tuning\\logs\\seed_0\\lightning_logs\n",
      "c:\\Users\\krkac\\Miniconda3\\envs\\tts\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\krkac\\Miniconda3\\envs\\tts\\lib\\site-packages\\pytorch_lightning\\utilities\\data.py:84: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 128. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "c:\\Users\\krkac\\Miniconda3\\envs\\tts\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\krkac\\Miniconda3\\envs\\tts\\lib\\site-packages\\pytorch_lightning\\utilities\\data.py:84: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 44. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "\u001b[32m[I 2023-04-23 17:53:26,593]\u001b[0m Trial 0 finished with value: 0.01742182858288288 and parameters: {'hidden_size_0': 78, 'hidden_size_1': 96, 'hidden_size_2': 84, 'activation': 'selu', 'dropout_p': 0.19172075941288885, 'lr': 0.0038322168504927897, 'batch_size': 128, 'weight_decay': 0.0005981221901152554}. Best is trial 0 with value: 0.01742182858288288.\u001b[0m\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: benchmarks\\2023-04-23T17-52-36\\TTS\\final\\logs\\seed_1826701614\\lightning_logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.11430840939283371\n",
      "[Best hyperparameter configuration]:\n",
      "{'hidden_size_0': 78, 'hidden_size_1': 96, 'hidden_size_2': 84, 'activation': 'selu', 'dropout_p': 0.19172075941288885, 'lr': 0.0038322168504927897, 'batch_size': 128, 'weight_decay': 0.0005981221901152554}\n",
      "[Tuning complete], saved tuning results to benchmarks\\2023-04-23T17-52-36\\TTS\\tuning\n",
      "[Training for 1 trials with best hyperparameters]\n",
      "[Training trial 1/1] seed: 1826701614\n",
      "train_loss: 0.01595594733953476\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_loss           0.05382462218403816\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krkac\\Miniconda3\\envs\\tts\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "from benchmark import repeat_benchmark\n",
    "\n",
    "timestamp = \"2023-04-23T17-49-27\"\n",
    "repeat_benchmark(timestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
